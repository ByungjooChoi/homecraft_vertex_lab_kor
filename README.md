# Homecraft Retail lab script:Elastic ESRE ì™€ Google GenAIë¥¼ ì´ìš©í•œ ì „ììƒê±°ë˜ ê²€ìƒ‰ app ë§Œë“¤ê¸°

ì´ ë¬¸ì„œëŠ” í•¸ì¦ˆì˜¨ ì‹¤ìŠµì„ ìœ„í•œ ìŠ¤í…ë°”ì´ìŠ¤í… ê°€ì´ë“œì´ë©°, ë‹¤ìŒ repoì—ì„œ ì‘ì„±ëœ ê²€ìƒ‰ app ì„ ì°¸ì¡°í•˜ê³  ìˆìŠµë‹ˆë‹¤. [Palm2 version](https://github.com/valerioarvizzigno/homecraft_vertex) / [Gemini version](https://github.com/valerioarvizzigno/homecraft_gemini). 

--- TESTED WITH ELASTIC CLOUD ELASTIC CLOUD 8.12 + ELAND 8.12 (and Elastic Cloud 8.8.1 + ELAND 8.3)   ---


## ì‚¬ì „ í™˜ê²½ ì„¤ì •

1. **Elastic Cloud Trial ì‹ ê·œ ê°€ì…**
   
   Elastic Cloud ë¥¼ ì‹ ê·œë¡œ ê°€ì…í•˜ì—¬ [free trial account](https://www.elastic.co/cloud/elasticsearch-service/signup) ë¥¼ ë§Œë“­ë‹ˆë‹¤
   ì‹ ê·œ ê³„ì • ìƒì„±ì‹œ gmail ê³„ì •ì„ ì‚¬ìš©í•˜ì‹œê³ , _HongKildong@gmail.com_ ì¼ ê²½ìš° "_HongKildong_**+codecamp**_@gmail.com_" ìœ¼ë¡œ ë§Œë“­ë‹ˆë‹¤
   
3. **í´ëŸ¬ìŠ¤í„° ì„¤ì •**

   ë‹¤ìŒê³¼ ë™ì¼í•˜ê²Œ Elastic í´ëŸ¬ìŠ¤í„°ë¥¼ ì„¤ì •í•˜ì„¸ìš”:
   - GCP region: "ğŸ‡°ğŸ‡·Â Seoul (asia-northeast3)", Elastic ë²„ì „ 8.12, Autoscaling "None" 
   - 1-zone 8GB (memory) Hot node (or 2-zone x 4GB)
   - 1-zone 4GB Machine Learning node
   - 1-zone 1GB Kibana node
   - 1-zone 4GB Enterprise Search
   - Leave everything else as it is by default
   - Create the cluster and download/note down the admin username/password
   - Explore the Kibana console and the management console
  
   (If on the Elastic trial, you're not able to specify topology on the initial screen, so create the cluster as per suggested default and then go to "Edit My Deployment" in the left panel -> "Actions" menu -> "Edit deployment" and set it as the list before. Don't worry if some node size doesn't match exactly the previous, just max out everything)

 <img width="646" alt="image" src="https://github.com/ByungjooChoi/homecraft_vertex_lab_kor/assets/48340524/b0e05462-e6c7-445d-9b7f-c34a94ae4c15">



  ![image](https://github.com/valerioarvizzigno/homecraft_vertex_lab/assets/122872322/7e11519d-1b73-4f19-93b2-bd7f166a72ca)



3. **ë¨¸ì‹ ëŸ¬ë‹ ë…¸ë“œ ì„¤ì •**

ì²« ë²ˆì§¸ ë‹¨ê³„ë¡œ, ìƒ‰ì¸í•  ì½˜í…ì¸ ì—ì„œ í…ìŠ¤íŠ¸ ì„ë² ë”©ì„ ìƒì„±í•˜ê¸° ìœ„í•´ Elastic ML ë…¸ë“œë¥¼ ì¤€ë¹„í•´ì•¼ í•©ë‹ˆë‹¤. ì„ íƒí•œ íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì„ Elasticì— ë¡œë“œí•˜ê³  ì‹œì‘í•˜ê¸°ë§Œ í•˜ë©´ ë©ë‹ˆë‹¤. ì´ ì‘ì—…ì€ [Eland Client](https://github.com/elastic/eland)ë¥¼ í†µí•´ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” [all-distillroberta-v1](https://huggingface.co/sentence-transformers/all-distilroberta-v1) ML ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ì´ëœë“œ í´ë¼ì´ì–¸íŠ¸ë¥¼ ì‹¤í–‰í•˜ë ¤ë©´ ë„ì»¤ê°€ ì„¤ì¹˜ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤. íŒŒì´ì¬/ë„ì»¤ ì„¤ì¹˜ ì—†ì´ ì´ ë‹¨ê³„ë¥¼ ì‰½ê²Œ ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ” ë°©ë²•ì€ êµ¬ê¸€ì˜ í´ë¼ìš°ë“œ ì…¸ì„ ì´ìš©í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ë³µì œí•˜ë ¤ëŠ” eland ë²„ì „ì´ ì„ íƒí•œ Elastic ë²„ì „ê³¼ í˜¸í™˜ë˜ëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”(ì˜ˆ: ì¼ë°˜ì ìœ¼ë¡œ eland 8.12ëŠ” elastic Cloud 8.12ì™€ ì •ìƒ ë™ì‘í•¨)! ìµœì‹  Elastic ë²„ì „ì„ ì‚¬ìš©í•˜ëŠ” ê²½ìš°, ì¼ë°˜ì ìœ¼ë¡œ ë³µì œí•˜ëŠ” ë™ì•ˆ Eland ë¦´ë¦¬ì¦ˆ ë²„ì „ì„ ì§€ì •í•  í•„ìš”ê°€ ì—†ìŠµë‹ˆë‹¤.
   - On Kibana (left panel) --> Stack Management --> Security --> Users ë©”ë‰´ì—ì„œ "elastic" ê³„ì •ì„ ë§Œë“¤ê³  "superuser" role ì„ ë¶™ì´ì„¸ìš”
     <img width="1102" alt="image" src="https://github.com/ByungjooChoi/homecraft_vertex_lab_kor/assets/48340524/94dd722e-5c7f-4f77-8487-046cba2927e6">

   - Google Cloud ì½˜ì†”ë¡œ ë“¤ì–´ê°€ì„¸ìš”
   - Cloud Shell ì—ë””í„°ë¥¼ ì—½ë‹ˆë‹¤ (ë§í¬ í´ë¦­ [this link](https://console.cloud.google.com/cloudshelleditor?cloudshell=true))
   - ë‹¤ìŒ ëª…ë ¹ì„ ì…ë ¥í•˜ì„¸ìš”. ë§ˆì§€ë§‰ ë¼ì¸ user ìƒì„±ì‹œ password ë¥¼ ì„ì˜ë¡œ ë³€ê²½í•œ ê²½ìš° ë°˜ë“œì‹œ ë³€ê²½í•œ password ë¡œ ë°”ê¾¸ì‹œê³ , ìì‹ ì˜ the elastic endpointë¡œ ë³€ê²½í•©ë‹ˆë‹¤(< > í‘œì‹œë„ ì‚­ì œ)
   - íŒ¨ìŠ¤ì›Œë“œì— ! ë‚˜ @ ë“± íŠ¹ìˆ˜ë¬¸ìê°€ ìˆì„ ê²½ìš°ì— \ ë¡œ escape ì²˜ë¦¬í•©ë‹ˆë‹¤

 ```bash
git clone https://github.com/elastic/eland.git #use -b vX.X.X option for specific eland version.

cd eland/

docker build -t elastic/eland .

docker run -it --rm elastic/eland eland_import_hub_model --url https://elastic:_codecamp_@<your_elastic_endpoint>:9243/ --hub-model-id sentence-transformers/all-distilroberta-v1 --start
 ```

   - Elatic admin home -> "Manage this deployment" ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ Elasticsearch ì˜ "Copy endpoint" ë¥¼ í´ë¦­í•˜ë©´ ìì‹ ì˜ endpoint ì£¼ì†Œë¥¼ í´ë¦½ë³´ë“œë¡œ ì¹´í”¼í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
   <img width="318" alt="image" src="https://github.com/ByungjooChoi/homecraft_vertex_lab_kor/assets/48340524/ff40e81a-1399-458d-9b5b-f5a087b8cf85">
   <img width="614" alt="image" src="https://github.com/ByungjooChoi/homecraft_vertex_lab_kor/assets/48340524/1bc3e1ee-04dd-43a6-85c6-7d76cb14c9a8">


  


4. **ML model í™•ì¸ ë° í…ŒìŠ¤íŠ¸**

ëª¨ë¸ì´ Elasticì— ë¡œë“œê°€ ì™„ë£Œë˜ë©´ ë°°í¬ë¥¼ ì…ë ¥í•˜ê³  ì™¼ìª½ ë©”ë‰´ì—ì„œ "Stack Management" -> "Machine Learning"ìœ¼ë¡œ ì´ë™í•©ë‹ˆë‹¤. all-distilroberta-v1 ëª¨ë¸ì´ ë³´ì´ê³  "started" ìƒíƒœì¸ ê²ƒì„ í™•ì¸í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤. "out of sync" ê²½ê³ ê°€ í‘œì‹œë˜ë©´ í•´ë‹¹ ê²½ê³ ë¥¼ í´ë¦­í•˜ê³  ë™ê¸°í™”í•©ë‹ˆë‹¤. ì´ì œ ëª¨ë“  ê²ƒì´ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤. ML ëª¨ë¸ì´ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤. ì´ì œ ìˆ˜ì§‘í•  ë¬¸ì„œì— íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì„ ì ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì›í•˜ëŠ” ê²½ìš° ê°™ì€ í˜ì´ì§€ì—ì„œ ëª¨ë¸ ì´ë¦„ ì˜¤ë¥¸ìª½ì— ìˆëŠ” ì  3ê°œ ë©”ë‰´ë¥¼ í´ë¦­í•˜ê³  "Test model"ì„ ì„ íƒí•˜ì—¬ í…ŒìŠ¤íŠ¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
   
   
5. **Ingest Part 1: The Web Crawler**
   
   Let's start with indexing some general retail data from the Ikea website. We will use the built-in web crawler feature, configure it this way:
   - Search for "Web Crawler" in the Elastic search bar
   - Name the new index as "search-homecraft-ikea" (watch out the "search-" prefix is already there) and go next screen.
   - Specify "https://www.ikea.com/gb/en/" in the domain URL field and click "Validate Domain". A warning should appear, saying that a robots.txt file was found. Click on it to open it in a separate browser tab and continue by clicking "Add domain"
   - For better crawling performance search the sitemap.xml filepath inside the robots.txt file of the target webserver, and add its path to the Site Maps tab.
   - To avoid crawling too many pages and stick with the english ones we can define some crawl rules. Set as follows (rule order counts!):
     
     ![image](https://github.com/valerioarvizzigno/homecraft_vertex_lab/assets/122872322/1f0d52cc-2d01-4b5d-9c0e-927042ccd932)

6. Now the new empty index should be automatically created for you. Have a look at it:
   - From the left panel, in the "Enterprise Search" section, select "Content" and click on index to explore its details.
   - You can also query it from the "Management" -> "Dev Tools" UI with the following query
     ```bash
     #Query index config
     GET search-homecraft-ikea 
     #Query index content
      GET search-homecraft-ikea/_search

7. **Creating an ingest pipeline for inference**
    
   Before start crawling we need to attach an ingest pipeline to the newly create index, so every time a new document is indexed, it will be processed by our ML model that will enrich the document with an additional field, the vector representation of its content.
   - Open the index from the Enterprise Search UI and navigate to the "Pipelines" tab
   - Click on "Copy and customize" to create a custom pipeline associated to this index
   - In the Machine Learning section click on "Add Inference Pipeline"
   - Name the pipeline as "ml-inference-title-vector"
   - Select your transformer model and go next screen
   - Select the "Source field". This is the field that the ML model will process to create vectors from, and the UI suggests the ones automatically created from the web crawler. Select the "title" field as source field, specify "title-vector" as target field, leave everything else as default and go on until pipeline is created.
  
8. Check the newly created ingest pipeline searching for it from the "Stack Management" -> "Ingest pipelines" section. You are able to analyze the processors (the processing tasks) listed in the pipeline and add/remove/modify them. Note also that you can specify exception handlers.
 
   ![image](https://github.com/valerioarvizzigno/homecraft_vertex_lab/assets/122872322/761cf843-c238-4f14-8fc2-47a6157f98b3)


8a. (Only if on Elastic 8.12, not the 8.3) As later we will be using "title-vector" field for kNN search in our application (and not the default "ml.inference.title-vector.predicted_values") let's add an additional SET processor at the bottom of the processors list as in the image below:

![image](https://github.com/valerioarvizzigno/homecraft_vertex_lab/assets/122872322/871c5b93-1a8e-4337-b98c-447456351c89)


Be aware of adding the following condition to handle errors:

```bash
ctx?.ml?.inference != null && ctx.ml.inference['title-vector'] != null
```

For extra safety also add a REMOVE processor at the beginning of the processors list, to remove any occasional presence of "title-vector" field names from sources, as following:

![image](https://github.com/valerioarvizzigno/homecraft_vertex_lab/assets/122872322/405bb7dd-80aa-48bf-aea8-ce8db5e0b607)


9. **Creating dense_vector mappings**
    
    Before launching the crawler we need to set the mappings for the target field where the vetors will be stored, specifying the "title-vector" field is of type "dense_vector", vector dimensions and its similarity algorithm. Let's execute the mapping API from the Dev Tools:

```bash
POST search-homecraft-ikea/_mapping
{
  "properties": {
    "title-vector": {
      "type": "dense_vector",
      "dims": 768,
      "index": true,
      "similarity": "dot_product"
    }
  }
}
```

10. **Launch the web crawler!**
    
    Start crawling: go back on the index and click on the "Start Crawling" button on the top right corner of the page. You will see the document count slowly increasing while the web crawler runs.

11. **Ingest Part 2: product catalog dataset upload**
    
    Let's now ingest a product catalog, to let our users search for products via hybrid search (keywords + semantics):
    - Load into Elastic this [Home Depot product catalog](https://www.kaggle.com/datasets/thedevastator/the-home-depot-products-dataset) via the "Upload File" feature (search for it in the top search bar). Click on "Import" and name the index "home-depot-product-catalog"
    - Take a look at the document structure, the fields available and their content. You will notice there are title and descriptions fields as well. We will apply the inference on the title field, so we can reuse the previously created ingest pipeline
    - From the Dev Tools console create a new empty index that will host the dense vectors called "home-depot-product-catalog-vector" and specify mappings.

```bash
PUT /home-depot-product-catalog-vector 

POST home-depot-product-catalog-vector/_mapping
{
  "properties": {
    "title-vector": {
      "type": "dense_vector",
      "dims": 768,
      "index": true,
      "similarity": "dot_product"
    }
  }
}
```

   - Re-index the product dataset through the same ingest pipeline previously created for the web-crawler. The new index will now have vectors embedded in documents in the title-vector field.

```bash
POST _reindex
{
  "source": {
    "index": "home-depot-product-catalog"
  },
  "dest": {
    "index": "home-depot-product-catalog-vector",
    "pipeline": "ml-inference-title-vector"
  }
}
```
  

  - (Note that we used these steps to show how to use reindexing and ingest pipelines via API. You can still apply the pipelines via UI as done before with the search-homecraft-ikea index)

12. **Ingest Part 3: data from BigQuery via Dataflow**
    
    To complete our data baseline we can also import an e-commerce order history. Leverage the BigQuery to Elasticsearch Dataflow's [native integration](https://www.elastic.co/blog/ingest-data-directly-from-google-bigquery-into-elastic-using-google-dataflow) to move this [sample e-commerce dataset](https://console.cloud.google.com/marketplace/product/bigquery-public-data/thelook-ecommerce?project=elastic-sa) into Elastic. Be aware the UI and available fields/label may visually change on Dataflow depending on the region you choose, but it should work regardlessly. This lab was tested with us-central1 region.
    - Click on "View Dataset" after reaching the Google Console with the previous link
    - Take a look at the tables available in this dataset within the BigQuery explorer UI. On the left panel open the "thelook_ecommerce" dataset and click on the "Order_items" table. Explore the content
    - Copy the ID (by clicking on the 3 dots) of the "Order_items" table and search for Dataflow in the Google Cloud service search bar
    - Create a new Dataflow job from template, selecting the "BigQuery to Elasticsearch" built-in template. You will need to specify:
       - The job's name
       - The region to run the job in (preferably choose the same you deployed the Elastic cluster in to minimize latencies and costs)
       - The Elastic cluster's "cloudID"
       - The Elastic API key to let Dataflow connect with Elastic(check [here](https://www.elastic.co/guide/en/kibana/current/api-keys.html) how to generate one)
       - The index name as "bigquery-thelook-order-items". Dataflow will automatically create this new index where all the table lines will be sent.
       - The dataset and table you want to read from (first field of the Optional Parameters): bigquery-public-data.thelook_ecommerce.order_items (some regions need bigquery-public-data:thelook_ecommerce.order_items instead - : after project name instead of .)
       - Elastic username and password
       - Launch the job
      
![image](https://github.com/valerioarvizzigno/homecraft_vertex_lab/assets/122872322/8708b87f-5855-447d-b6de-db7632a515cb)


This will let users to submit queries for their past orders. We are not creating embeddings on this index because keyword search will suffice (e.g. searching for userID, productID). Wait for the job to complete and check the data is correctly indexed in Elastic from the indices list in the UI.

13. **Test Vector Search**
    
    Let's test a kNN query directly from the Kibana Dev Tools. We will later see this embedded in our app. You can copy paste the following query from [this file](https://github.com/valerioarvizzigno/homecraft_vertex/blob/main/Additional%20sources/helpful_elastic_api_calls).

![image](https://github.com/valerioarvizzigno/homecraft_vertex_lab/assets/122872322/1a6e0079-7b51-4791-adfe-6b863037a9b5)

We are executing the classic "_search" API call, to query documents in Elasticsearch. The Semantic Search component is provided by the "knn" clause, where we specify the field we want to search vectors into (title-vector), the number of relevant documents we want to extract and the user provided query. Note that, to compare vectors also the user query has to be translated into text-embeddings from our ML model. We are then specifying the "text_embedding" field in the API call: this will let create vectors on-the-fly on the user query and compare them with the stored documents.
The "query" clause, instead, will enable keyword search on the same elastic index. This way we are using both techniques but still receiving an unified output.
      
14. **Deploying the search app front-end on Compute Engine**
    
    We are now going to deploy our front-end app. Create a small Google Cloud Compute Engine linux machine with default settings, with public IPv4 address enabled (set it in the Advanced Settings -> Networking), HTTP and HTTPS traffic enabled (tick the checkboxes) and access it via SSH. This will be used as our web-server for the front-end application, hosting our "intelligent search bar". We suggest these settings:
    - e2-medium
    - Debian11  

15. Update the machine, install git, python and pip.
```bash
sudo apt-get update
sudo apt install git #Press Y if prompted
sudo apt install pip #Press Y if prompted

#Check if python is installed (it probably is).
python3 --version
#If not python version is found install it
sudo apt install python3

```

16. Clone the [homecraft_gemini source-code repo](https://github.com/valerioarvizzigno/homecraft_gemini) on your CE machine.

```bash
git clone https://github.com/valerioarvizzigno/homecraft_gemini.git
```

17. **Configure VertexAI and Elasticsearch SDKs**
    
     Install requirements needed to run the app from the requirements.txt file. After this step, close the SSH session and reopen it, to ensure all the $PATH variables are refreshed (otherwise you can get a "streamlit command not found" error). You can have a look at the requirements.txt file content via vim/nano or your favourite editor.

```bash
cd homecraft_vertex
pip install -r requirements.txt

```

18. Authenticate the machine to the VertexAI SDK (installed via the requirements.txt file) and follow the instructions.
```bash
    gcloud auth application-default login
```

19. Set up the environment variables cloud_id (the elastic CloudID - find it on the Elastic admin console), cloud_pass and cloud_user (Elastic deployments's user details) and gcp_project_id (the GCP project you're working in). This variables are used inside of the app code to reference the correct systems to communicate with (Elastic cluster and VertexAI API in your GCP project)

```bash
export cloud_id='<replaceHereYourElasticCloudID>'
export cloud_user='elastic'
export cloud_pass='<replaceHereYourElasticDeploymentPassword>'
export gcp_project_id='<replaceHereTheGCPProjectID>'
```

20. **Run the app!**
    
    Run the app and access it copying the public IP:PORT the console will display in a web browser page
```bash
streamlit run homecraft_home.py
```

21. Test your app!

## Web-app code ì„¤ëª…

genAI ëª¨ë¸ì´ ì–´ë–»ê²Œ ì‚¬ìš©ë˜ëŠ”ì§€, ê·¸ë¦¬ê³  VertexAI ë° Gemini Pro ëª¨ë¸ê³¼ í†µí•©í•˜ëŠ” ë°©ë²•ì„ ì™„ì „íˆ ì´í•´í•˜ë ¤ë©´ ì›¹ ì•± ì½”ë“œë¥¼ ì‚´í´ë³´ì„¸ìš”. ì´ ì• í”Œë¦¬ì¼€ì´ì…˜ì€ "streamlit" íŒŒì´ì¬ í”„ë ˆì„ì›Œí¬ë¡œ êµ¬ì¶•ë˜ì–´ í”„ë¡œí† íƒ€ì´í•‘ê³¼ ë°ëª¨ë¥¼ ìœ„í•œ ê°„ë‹¨í•˜ê³  ë¹ ë¥¸ í”„ë¡ íŠ¸ì—”ë“œ ê°œë°œì´ ê°€ëŠ¥í•©ë‹ˆë‹¤

ëŒ€ë¶€ë¶„ì˜ ì½”ë“œëŠ” ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ì‹œì‘í•˜ê¸° ìœ„í•´ "run" commandì—ì„œ ì°¸ì¡°í•œ "homecraft_home.py" íŒŒì¼ì— ìˆìŠµë‹ˆë‹¤. ì¶”ê°€ í˜ì´ì§€ëŠ” "pages" í´ë”ì—ì„œ ì°¾ì„ ìˆ˜ ìˆìœ¼ë©°, ì´ í´ë”ì˜ ë¼ìš°íŒ…ì€ streamlit ì—ì„œ ìë™ìœ¼ë¡œ ê´€ë¦¬í•©ë‹ˆë‹¤

ì†ŒìŠ¤ ì½”ë“œë¥¼ ê°„ëµí•˜ê²Œ ì„¤ëª…í•˜ê² ìŠµë‹ˆë‹¤:

- ì²« ë²ˆì§¸ ì¤„ì€ í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ê°€ì ¸ì˜¤ëŠ” ê²ƒì…ë‹ˆë‹¤. ì•± êµ¬ì¡°ë¥¼ ìœ„í•œ "streamlit", API í˜¸ì¶œê³¼ Elastic í´ëŸ¬ìŠ¤í„°ì™€ì˜ ì—°ê²°ì„ ì¶”ìƒí™”í•˜ê¸° ìœ„í•œ "elasticsearch", ê·¸ë¦¬ê³  ëª¨ë“  GenAI ëª¨ë¸ ë° ë„êµ¬ì™€ ìƒí˜¸ ì‘ìš©í•˜ê¸° ìœ„í•œ Googleì˜ SDKì¸ "vertexAI"ë¥¼ ê°€ì ¸ì˜¤ëŠ” ê²ƒì…ë‹ˆë‹¤. ì•ì„œ ì‚´í´ë³¸ ë°”ì™€ ê°™ì´, ìš°ë¦¬ ì•±ì´ VertexAI APIë¥¼ í˜¸ì¶œí•  ìˆ˜ ìˆë„ë¡ í•˜ê¸° ìœ„í•´ Google ì„œë¹„ìŠ¤ì— ëŒ€í•œ í”„ë¡œê·¸ë˜ë° ë°©ì‹ì˜ ì•¡ì„¸ìŠ¤ë¥¼ ìœ„í•´ ë¨¸ì‹ ì„ ì¸ì¦í•´ì•¼ í•©ë‹ˆë‹¤.
- The first lines will be importing the requried libraries. "streamlit" for the app structure, "elasticsearch" for abstracting the API calls and connections with the Elastic cluster, and "vertexAI", the Google's SDK for interacting with all the GenAI models and tools. As previously seen, we had to authenticate our machine for programmatic access to Google services to let our app call VertexAI APIs.

```bash
import os
import pandas as pd
from io import StringIO
import streamlit as st
from elasticsearch import Elasticsearch
import vertexai
from vertexai.preview.generative_models import GenerativeModel, ChatSession, GenerationConfig, Image, Part
```

- ë‹¤ìŒìœ¼ë¡œ Elasticê³¼ GCP í”„ë¡œì íŠ¸ì˜ ìê²© ì¦ëª…/ì—”ë“œí¬ì¸íŠ¸ì— ëŒ€í•´ ì´ì „ì— ì •ì˜í•œ í™˜ê²½ ë³€ìˆ˜ë¥¼ ì½ëŠ” ë‚´ë¶€ ë³€ìˆ˜ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ë³€ìˆ˜ëŠ” ë‚˜ì¤‘ì— ì—°ê²°ì„ ì„¤ì •í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤.

```bash
projid = os.environ['gcp_project_id']
cid = os.environ['cloud_id']
cp = os.environ['cloud_pass']
cu = os.environ['cloud_user']
```

- ë‹¤ìŒ ì„¤ì •ìœ¼ë¡œ GenAI ëª¨ë¸ì„ êµ¬ì„±í•©ë‹ˆë‹¤. ì‚¬ìš©í•  ëª¨ë¸, ëª¨ë¸ì˜ ì°½ì˜ì„± ë° ì¶œë ¥ ê¸¸ì´ë¥¼ ì •ì˜í•˜ê³  SDKë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
- With the following settings we are configuring our GenAI model. We define which model we want to use, model's creativity and output lenght and we initialize the sdk.

```bash
generation_config = GenerationConfig(
    temperature=0.4, # 0 - 1. The higher the temp the more creative and less on point answers become
    max_output_tokens=2048, #modify this number (1 - 1024) for short/longer answers
    top_p=0.8,
    top_k=40,
    candidate_count=1,
)
vertexai.init(project=projid, location="us-central1")
model = GenerativeModel("gemini-pro")
visionModel = GenerativeModel("gemini-1.0-pro-vision-001")
```

- line 50 ë¶€í„° line 185 ê¹Œì§€ëŠ” Elastic í´ëŸ¬ìŠ¤í„°ì™€ì˜ ì—°ê²°ì„ ì„¤ì •í•˜ê³  ì„¸ ê°€ì§€ ì£¼ìš” API í˜¸ì¶œì„ ì‹¤í–‰í•˜ëŠ” ë©”ì„œë“œë¥¼ ì •ì˜í•©ë‹ˆë‹¤. 
a. search_products: ì‹œë§¨í‹± ê²€ìƒ‰ì„ ì‚¬ìš©í•˜ì—¬ HomeDepot ë°ì´í„° ì„¸íŠ¸ì—ì„œ ì œí’ˆ ì°¾ê¸°
b. search_docs: ì‹œë§¨í‹± ê²€ìƒ‰ì„ ì‚¬ìš©í•˜ì—¬ Ikea ì›¹ í¬ë¡¤ë§ ì¸ë±ìŠ¤ì—ì„œ ì¼ë°˜ ì†Œë§¤ ì •ë³´ ì°¾ê¸°
c. search_orders: í‚¤ì›Œë“œ ê²€ìƒ‰ìœ¼ë¡œ ê³¼ê±° ì‚¬ìš©ì ì£¼ë¬¸ ê²€ìƒ‰í•˜ê¸°

- ì§„ì§œ ë§ˆë²•ì€ ë‹¤ìŒ ì½”ë“œì—ì„œ ì¼ì–´ë‚©ë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” ì›¹ ì–‘ì‹ì—ì„œ ì‚¬ìš©ì ì…ë ¥ì„ ìº¡ì²˜í•˜ê³ , Elasticì—ì„œ ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ì‹¤í–‰í•˜ê³ , ì‘ë‹µì„ ì‚¬ìš©í•˜ì—¬ ë¯¸ë¦¬ ì •ì˜ëœ í”„ë¡¬í”„íŠ¸ì— ë³€ìˆ˜ë¥¼ ì±„ì›Œ GenAI ëª¨ë¸ì— ì „ì†¡ë˜ë„ë¡ í•©ë‹ˆë‹¤.

```bash
# Generate and display response on form submission
negResponse = "I'm unable to answer the question based on the information I have from Homecraft dataset."
if submit_button:
    queryForElastic = ''
    answerVision = ''
    if uploaded_file is not None:
        visionQuery = 'What is in the picture?'
        answerVision = generateVisionResponse(visionQuery,uploaded_file)
        #To Elastic, for semantic search, we send both the question and the first answer from the vision model
        queryForElastic = query + answerVision 
        st.write(f"**Vision assistant answer:**  \n\n{answerVision.strip()}")
    
    es = es_connect(cid, cu, cp)
    resp_products, url_products = search_products(query if queryForElastic == '' else queryForElastic)
    resp_docs, url_docs = search_docs(query if queryForElastic == '' else queryForElastic)
    resp_order_items = search_orders(1) # 1 is the hardcoded userid, to simplify this scenario. You should take user_id by user session
    #prompt = f"You are an e-commerce AI assistant. Answer this question: {query} using this context: \n{resp_products} \n {resp_docs} \n {resp_order_items}."
    #prompt = f"You are an e-commerce AI assistant. Answer this question: {query}.\n If product information is requested use the information provided in this JSON: {resp_products} listing the identified products in bullet points with this format: Product name, product key features, price, web url. \n For other questions use the documentation provided in these docs: {resp_docs} and your own knowledge. \n If the question contains requests for user past orders consider the following order list: {resp_order_items}"
    prompt = [f"You are an e-commerce AI assistant.", f"You answer question around product catalog, general company information and user past orders", f"Answer this question: {query}.", f"Context: Picture content = {answerVision}; Product catalog = {resp_products}; General information = {resp_docs}; Past orders = {resp_order_items} "]
    #prompt = f"You are an e-commerce customer AI assistant. Answer this question: {query}.\n with your own knowledge and using the information provided in the context. Context: JSON product catalog: {resp_products} \n, these docs: \n {resp_docs} \n and this user order history: \n {resp_order_items}"
    #answer = vertexAI(chat, prompt)
    answer = generateResponse(prompt)

    if answer.strip() == '':
        st.write(f"**Elastic-powered Assistant:** \n\n{answer.strip()}")
    else:
        st.write(f"**Elastic-powered Assistant:** \n\n{answer.strip()}\n\n")
        #st.write(f"Order items: {resp_order_items}")
```

## Sample questions

### Following questions are for text only Gemini

í•˜ê¸° ì§ˆë¬¸ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ í•´ë³´ì„¸ìš”: 

- "List the 3 top paint primers in the product catalog, specify also the sales price for each product and product key features. Then explain in bullet points how to use a paint primer".
You can also try asking for related urls and availability --> leveraging private product catalog + public knowledge

- "could you please list the available stores in UK" --> --> it will likely use (crawled docs)

- "Which are the ways to contact customer support in the UK? What is the webpage url for customer support?" --> it will likely use crawled docs

- Please provide the social media accounts info from the company --> it will likely use crawled docs

- Please provide the full address of the Manchester store in UK --> it will likely use crawled docs

- are you offering a free parcel delivery? --> it will likely use crawled docs

- Could you please list my past orders? Please specify price for each product --> it will search into BigQuery order dataset
  
- After uploading the image (find some in the samples/images folder) try queries like this one:

- List all the items I have bought in my order history in bullet points


### Following questions are for multimodal Gemini

After uploading the image (find some in the samples/images folder) try queries like this one:

- What's in the image? Do you have similar products in your catalog? If yes list them with descriptions and prices
